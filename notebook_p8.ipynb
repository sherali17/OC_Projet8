{"cells":[{"cell_type":"markdown","source":["# PROJET 8 : \"Déployer un modèle dans le cloud\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bc871b9-466b-4999-a438-d81866b6dab8"}}},{"cell_type":"markdown","source":["Les objectifs de ce notebook Databricks sont :\n  - connecter Databricks avec une instance de stockage AWS S3\n  - charger les images stockées sur AWS S3\n  - effectuer un préprocessing de ces images\n  - effectuer une réduction de dimension\n  - écrire le résultat de ce processing dans un fichier accessible sur AWS S3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b539621b-01a1-4cfe-8811-12d2134b608c"}}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75b0c603-2272-4e81-b689-b8994234704e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# pyspark functions\nfrom pyspark.sql.functions import *\n# URL processing\nimport urllib\n\n# Chargement des librairies\nimport datetime\nimport io\nimport sys\nimport time\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n# Visualisation\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pyspark\nimport pyspark\nfrom pyspark.sql.functions import element_at, split, col, pandas_udf, PandasUDFType, udf\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql import SparkSession\n\n\n# Gestion des images\nimport PIL\nfrom PIL import Image, ImageOps\n\n# Taches ML\nfrom pyspark.ml.image import ImageSchema\nimport tensorflow as tf\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom pyspark.sql.functions import col, pandas_udf, PandasUDFType\n\nfrom pyspark.sql.functions import split, element_at\n\n# Réduction de dimension - PCA\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.feature import StandardScaler\n\n# Conversion des données en vecteurs\nfrom pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\nfrom pyspark.sql.functions import udf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61795f12-15bf-4bfa-9456-ff80d4e27d18"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 1. Connecter AWS S3 avec Databricks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30fe2beb-da56-4aff-9f0f-cf7478eaa0d6"}}},{"cell_type":"markdown","source":["##### 1.1. Chargement du fichier \"credentials\" que nous avons téléchargé depuis AWS IAM et contenant ID de connection et le code secret pour se connecter au bucket AWS S3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1338088a-35bf-4337-a4d1-27d9dbe56fc2"}}},{"cell_type":"code","source":["# Check the contents in tables folder\ndbutils.fs.ls(\"/FileStore/tables\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a58503c-6d75-4293-9923-aa71c1f6c9de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: [FileInfo(path='dbfs:/FileStore/tables/credentials.csv', name='credentials.csv', size=200),\n FileInfo(path='dbfs:/FileStore/tables/house_prices.csv', name='house_prices.csv', size=2016),\n FileInfo(path='dbfs:/FileStore/tables/new_user_credentials-1.csv', name='new_user_credentials-1.csv', size=207),\n FileInfo(path='dbfs:/FileStore/tables/new_user_credentials.csv', name='new_user_credentials.csv', size=205)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: [FileInfo(path='dbfs:/FileStore/tables/credentials.csv', name='credentials.csv', size=200),\n FileInfo(path='dbfs:/FileStore/tables/house_prices.csv', name='house_prices.csv', size=2016),\n FileInfo(path='dbfs:/FileStore/tables/new_user_credentials-1.csv', name='new_user_credentials-1.csv', size=207),\n FileInfo(path='dbfs:/FileStore/tables/new_user_credentials.csv', name='new_user_credentials.csv', size=205)]"]}}],"execution_count":0},{"cell_type":"code","source":["# Define file type\nfile_type = \"csv\"\n# Whether the file has a header\nfirst_row_is_header = \"true\"\n# Delimiter used in the file\ndelimiter = \",\"\n# Read the CSV file to spark dataframe\naws_keys_df = spark.read.format(file_type)\\\n.option(\"header\", first_row_is_header)\\\n.option(\"sep\", delimiter)\\\n.load(\"/FileStore/tables/credentials.csv\")\n#display(aws_keys_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a9128df-1e2a-4c34-8c01-29ab49eb004f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 1.2. Extraction de l'ID de connexion et le code secret du dataframe spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4304db32-58a8-46d0-80c3-7dc32b5dd45b"}}},{"cell_type":"code","source":["dbutils.fs.unmount(\"/mnt/dbp8v2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cae6f66e-9241-4f26-89fa-18fafdb89948"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/mnt/dbp8v2 has been unmounted.\nOut[7]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/mnt/dbp8v2 has been unmounted.\nOut[7]: True"]}}],"execution_count":0},{"cell_type":"code","source":["# Get the AWS access key and secret key from the spark dataframe\nACCESS_KEY = aws_keys_df.where(col('User name')=='databricks_P8').select('Access key ID').collect()[0]['Access key ID']\nSECRET_KEY = aws_keys_df.where(col('User name')=='databricks_P8').select('Secret access key').collect()[0]['Secret access key']\n# Encode the secrete key\nENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94efb014-19e4-46fa-ae0a-8a1bf1bfe696"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 1.3. Etablissement de la connexion entre Databricks et le bucket AWS S3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48110157-ecef-418e-826c-4bf7c24415da"}}},{"cell_type":"code","source":["# AWS S3 bucket name\nAWS_S3_BUCKET = \"dbp8v2\"\n# Mount name for the bucket\nMOUNT_NAME = \"/mnt/dbp8v2\"\n# Source url\nSOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n# Mount the drive\ndbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a817601-a932-4ea3-9c64-ee9ad7c33c19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: True"]}}],"execution_count":0},{"cell_type":"code","source":["# Check if the AWS S3 bucket was mounted successfully\ndisplay(dbutils.fs.ls(\"/mnt/dbp8v2/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ccca044-23d9-48cf-a54e-59f3f0d9881d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/dbp8v2/output_features_parquet/","output_features_parquet/",0],["dbfs:/mnt/dbp8v2/training_sample/","training_sample/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/dbp8v2/output_features_parquet/</td><td>output_features_parquet/</td><td>0</td></tr><tr><td>dbfs:/mnt/dbp8v2/training_sample/</td><td>training_sample/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2. Chargement et lecture des données stockées sur le bucket AWS S3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f18ce6c4-86a8-40cf-ac74-efb51ba6ac2b"}}},{"cell_type":"code","source":["# Chemin de stockage des images du jeu de données\npath_train_set = \"/mnt/dbp8v2/training_sample/*/*\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7d338c2-d903-4877-bf0e-e329d773f3e1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Chargement des images du train set au format \"binaryFile\"\ndf_binary_train = spark.read.format(\"binaryFile\") \\\n  .option(\"pathGlobFilter\", \"*.jpg\") \\\n  .option(\"recursiveFileLookup\", \"true\") \\\n  .load(path_train_set)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97b9b51d-ffa1-48c6-8748-5cec5c7c379c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Nombre d'images? (pour ce notebook j'ai pris un sample des données)\ndf_binary_train.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96092a60-2db2-42c9-a503-28a4e601b565"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[13]: 1263","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: 1263"]}}],"execution_count":0},{"cell_type":"code","source":["# Affichage des premières lignes du  dataframe contenant les images\ndf_binary_train.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"add19e21-e22d-4c9c-9a08-e67acf582d1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+-------------------+------+--------------------+\n|                path|   modificationTime|length|             content|\n+--------------------+-------------------+------+--------------------+\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:27|106833|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:23|104151|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:29|104055|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:24|103763|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:21|102207|[FF D8 FF E0 00 1...|\n+--------------------+-------------------+------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+-------------------+------+--------------------+\n|                path|   modificationTime|length|             content|\n+--------------------+-------------------+------+--------------------+\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:27|106833|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:23|104151|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:29|104055|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:24|103763|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|2022-01-14 04:40:21|102207|[FF D8 FF E0 00 1...|\n+--------------------+-------------------+------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3. Processing des données"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"261cfcdd-5bbb-4f1f-b5f6-5ec506885ec3"}}},{"cell_type":"markdown","source":["##### 3.1. Extraction des features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bd2de3d-1de1-47a3-9f31-2d66b371a100"}}},{"cell_type":"markdown","source":["* Plusieurs méthodes existent pour extraires les features les plus importantes des images (parmis lesquelles les méthodes ORB, SIFT, SURF, modèle de transfert learning)\n* Nous allons utiliser sur algorithme Resnet50 pré-entrainé sur ImageNet afin d'extraire les features de nos images\n* Pour cela nous supprimerons la dernière couche (softmax) de notre algorithme (à l'aide du paramètre : include_top=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"646c199f-db85-4a06-bf3b-ced936b939ff"}}},{"cell_type":"code","source":["# Préparation du dataframe\ndf_images = df_binary_train.select(\"path\", \"content\")\ndf_images.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e90b5025-3571-4300-9232-6fabd125434c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+--------------------+\n|                path|             content|\n+--------------------+--------------------+\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+--------------------+\n|                path|             content|\n+--------------------+--------------------+\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n|dbfs:/mnt/dbp8v2/...|[FF D8 FF E0 00 1...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###### 3.2.1. Préparation du modèle ResNet50"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"565651fb-b950-4e82-b1ef-055b723bcf39"}}},{"cell_type":"markdown","source":["* Utilisation d'un ResNet50 pré-entrainé\n* On retire la couche fully-connected (include_top=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43872a23-575c-4e7d-b032-30012e2eef74"}}},{"cell_type":"code","source":["# Instanciation du modèle\nmodel = ResNet50(include_top=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7eb9660-8f0c-4738-9741-497133083056"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\r   16384/94765736 [..............................] - ETA: 11s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  385024/94765736 [..............................] - ETA: 12s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5701632/94765736 [>.............................] - ETA: 1s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13418496/94765736 [===>..........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r21200896/94765736 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28917760/94765736 [========>.....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r36675584/94765736 [==========>...................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44138496/94765736 [============>.................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r51724288/94765736 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r58933248/94765736 [=================>............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r66633728/94765736 [====================>.........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r74121216/94765736 [======================>.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r81690624/94765736 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r88637440/94765736 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r94773248/94765736 [==============================] - 1s 0us/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r94781440/94765736 [==============================] - 1s 0us/step\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\r   16384/94765736 [..............................] - ETA: 11s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  385024/94765736 [..............................] - ETA: 12s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5701632/94765736 [>.............................] - ETA: 1s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13418496/94765736 [===>..........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r21200896/94765736 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28917760/94765736 [========>.....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r36675584/94765736 [==========>...................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44138496/94765736 [============>.................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r51724288/94765736 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r58933248/94765736 [=================>............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r66633728/94765736 [====================>.........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r74121216/94765736 [======================>.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r81690624/94765736 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r88637440/94765736 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r94773248/94765736 [==============================] - 1s 0us/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r94781440/94765736 [==============================] - 1s 0us/step\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Permettre aux workers Spark d'accéder aux poids utilisés par le modèle\nbc_model_weights = sc.broadcast(model.get_weights())\n\n# On utilise le broadcast de SparkContext pour partager dans le cluster des valeurs  (ICI les poids du modéle) en lecture-seule\n# Cela permet de réduire les coûts de communication \n\ndef model_fn():\n    \"\"\"\n    Retourne un modèle ResNet50 avec la couche supérieure enlevée (fully-connected) et \n    des pondérations en broadcast déjà pré-entraînés. \n    On retire la couche supérieure car c'est celle qui permet de faire une classification, or \n    ici on se sert du modéle pour extraire des features.\n    \n    Pooling = \"avg\" est utilisé pour réduire le nombre de caractéristiques à 2048\n    \"\"\"\n\n    model = ResNet50(weights=None, include_top=False, pooling='avg')\n    #on ajoute les pondérations\n    model.set_weights(bc_model_weights.value)\n    \n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"381cadb9-123b-4188-b682-e20840cbbedd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def preprocess(content):\n    \"\"\"\n    Fonction de pre-processing d'images.\n    \"\"\"\n    # Redimensionnement de l'image : Resnet 50 ne prend en charge que des images de taille 224x224\n    img = Image.open(io.BytesIO(content)).resize([224, 224])   \n    \n    # Changer le type d'image en matrice  \n    img2 = np.asarray(img) \n    \n     # Préparation au pre-process de Keras\n    arr = img_to_array(img2)\n    return preprocess_input(arr) \n  \n  \ndef featurize_series(model, content_series):\n    \"\"\"\n    Retourne un pd.Series des features de l'image\n    \"\"\"\n    \n    input = np.stack(content_series.map(preprocess))\n  # Extraction des features des images\n    preds = model.predict(input)\n  # Pour certaines couches, les caractéristiques de sortie sont des tensors multidimensionnels\n  # On aplatit les caractéristiques de tensors en vecteurs pour faciliter le stockage dans les dataframes Spark\n  # la fonction flatten() envoie une copie du tableau réduit à une seule dimension \n    output = [p.flatten() for p in preds]\n    return pd.Series(output)  \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86ef7d2c-c09a-4ca4-9a28-818373490eda"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@pandas_udf('array<double>', PandasUDFType.SCALAR_ITER)\n\ndef featurize_udf(content_series_iter):\n    '''\n    Cette méthode est un Itérateur Scalaire (pandas UDF signifiant User-Defined Functions) qui complète\n    la fonction de featurisation.\n    Cela renvoie une colonne Spark DataFrame de type ArrayType(FloatType).\n  \n    param content_series_iter : Cet argument est un itérateur sur des lots de données, où chaque lot\n    est une série de données d'images pandas.\n    '''\n    \n  # Avec les pandas UDF Scalar Iterator, on peut charger le modèle une fois et le réutiliser ensuite\n  # pour plusieurs lots de données. Cela permet d'amortir les frais liés au chargement de grands modèles.\n    model = model_fn()\n    for content_series in content_series_iter:\n        yield featurize_series(model, content_series)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"caf306b8-e92a-4fff-a4bb-f166d65752f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n"]}}],"execution_count":0},{"cell_type":"code","source":["# On lance la recherche de features sur notre dataframe Spark \n\ndf_features = df_images.repartition(4).select(col(\"path\"), featurize_udf(\"content\").alias(\"features\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"952386fa-bcae-4180-8ba5-1296bab44cd5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 3.4. Labélisation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e119eaf-8249-49b5-b4de-be3139e9910b"}}},{"cell_type":"markdown","source":["* Il est mportant de créer une colonne label dans notre dataframe car ces données serviront d'entrée à des algorithmes de classication.\n* Nous pouvons constater que la classe de l'image est définie dans le nom de son répertoire.\n* Nous allons donc extraire le nom de classe depuis chaque répertoire."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5c61061-1122-4aad-b364-4a96b8e6df04"}}},{"cell_type":"code","source":["# Ajout dans la colonne label pour chaque image traitée de l'avant dernier élément du nom du répertoire de stockage de l'image==>df_binary_train[\"path\"]\n# On ajoute une colonne 'label' au dataframe des features\n\n#on récupére le type de fruit à partir du chemin de l'image.\ndf_features_label = df_features.withColumn('label', element_at(split(df_features['path'],\"/\"),-2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c05d14c9-11f6-4504-b2bc-d436fb11d7d3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Visualisation des 5 premières images avec la classe\ndf_features_label.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9e18699-839d-4a9d-aec8-75e3af422a48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+--------------------+---------------+\n|                path|            features|          label|\n+--------------------+--------------------+---------------+\n|dbfs:/mnt/dbp8v2/...|[0.73054784536361...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[1.64908504486083...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[1.95849287509918...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[0.54736912250518...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[0.58331966400146...|cabbage_white_1|\n+--------------------+--------------------+---------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+--------------------+---------------+\n|                path|            features|          label|\n+--------------------+--------------------+---------------+\n|dbfs:/mnt/dbp8v2/...|[0.73054784536361...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[1.64908504486083...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[1.95849287509918...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[0.54736912250518...|cabbage_white_1|\n|dbfs:/mnt/dbp8v2/...|[0.58331966400146...|cabbage_white_1|\n+--------------------+--------------------+---------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4. Réduction de dimension avec une Analyse en composantes principales (PCA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d238689f-9a1f-4c2f-a765-e1f5b999d43d"}}},{"cell_type":"markdown","source":["##### 4.1. Préparation des données (conversion des features en vecteurs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f41209a1-3e73-4432-b832-49555b3c46dd"}}},{"cell_type":"code","source":["#Construction d'une User defined Fonction qui transforme les listes de features en vecteurs denses.\nlist_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n\n# On applique cette UDF à la colonne \"features\" de notre dataframe \ndf_with_vectors = df_features_label.select( df_features_label[\"path\"],\n                                            df_features_label[\"label\"],\n                                           list_to_vector_udf( df_features_label[\"features\"]).alias(\"features\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"434d0a3f-771e-4ac7-ab45-24c409d39cfe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 4.2. Détermination du nombre optimal de composantes principales"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73ef51f5-823f-4e64-afb5-3a1326034213"}}},{"cell_type":"code","source":["# Recherche du nombre optimal de dimensions pour la réduction dimensionnelle des features\n\nnum_components = 2048\npca_opt = PCA(k = num_components,\n         inputCol=\"features\", \n         outputCol=\"features_pca\")\n\nmodel = pca_opt.fit(df_with_vectors)\nscree = model.explainedVariance# Pré-processing (vecteur dense, standardisation)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad5c78fa-5730-492a-a4fd-2f28cc71737e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Scree plot pour déterminer le nombre optimal de composantes principales\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.figure(figsize=(11,6))\nplt.bar(np.arange(len(scree))+1, scree)\nplt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\nplt.xlabel(\"Nombre de composantes principales\")\nplt.ylabel(\"Variance expliquée en pourcentage\")\nplt.title(\"Scree plot\")\nplt.grid()\nplt.show(block=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7169706-f506-41c6-89f1-d2d9cbcefe6f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 4.2. Recherche du nombre composantes expliquant 95 % de la variance"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"018ec043-9090-40a3-9982-c165a573c7db"}}},{"cell_type":"code","source":["# On chercher à  trouver le nombre de composantes principales à conserver.\n# On se fixe ici de converser au moins 95 % de l'inertie .\nnbr_pca=0\nfor i in range(50):\n    a = scree.cumsum()[i]\n    if a >= 0.95:\n        print(\"{} composantes principales expliquent au moins 95% de la variance totale\".format(i))\n        print(\"Valeur exacte de variance expliquée:{}%\".format(a*100))\n        nbr_pca=i\n        break"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df16ac85-b94c-47d5-a137-30e26ecc7ddb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 4.3. Réduction de dimension suivant le nombre de dimensions choisies (k)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61289abc-5de1-4477-8acb-28fcd8bf9430"}}},{"cell_type":"code","source":["# On pratique la PCA  sur la colonne \"Feature\" de notre dataframe et on recuperera le resultat dans une nouvelle colonne \"Feature_pca\"\npca = PCA(k=40,inputCol=\"features\",outputCol=\"features_pca\")\n\n# Entraînement du PCA\nmodel = pca.fit(df_with_vectors)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c18c068-d506-44af-b1f9-3a3f3fd43cac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Dataframe post-PCA\n\ndf_pca = model.transform(df_with_vectors)\ndf_pca.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a66f8e29-257f-41a5-9d08-5d516a78a675"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+---------------+--------------------+--------------------+\n|                path|          label|            features|        features_pca|\n+--------------------+---------------+--------------------+--------------------+\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[0.73054784536361...|[2.58817404745408...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[1.64908504486083...|[3.21223116140212...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[1.95849287509918...|[1.89440665733666...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[0.54736912250518...|[2.68806227425664...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[0.58331966400146...|[2.28526925334920...|\n+--------------------+---------------+--------------------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+---------------+--------------------+--------------------+\n|                path|          label|            features|        features_pca|\n+--------------------+---------------+--------------------+--------------------+\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[0.73054784536361...|[2.58817404745408...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[1.64908504486083...|[3.21223116140212...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[1.95849287509918...|[1.89440665733666...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[0.54736912250518...|[2.68806227425664...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[0.58331966400146...|[2.28526925334920...|\n+--------------------+---------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Dataframe final avec chemin vers images, labels et features obtenus par PCA (on enlève la colonne features)\n\ndf_final = df_pca[['path','label','features_pca']]\ndf_final.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bcbd49b-91f0-4b56-941d-cb3841987424"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+---------------+--------------------+\n|                path|          label|        features_pca|\n+--------------------+---------------+--------------------+\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[2.58817404745408...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[3.21223116140212...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[1.89440665733666...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[2.68806227425664...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[2.28526925334920...|\n+--------------------+---------------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+---------------+--------------------+\n|                path|          label|        features_pca|\n+--------------------+---------------+--------------------+\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[2.58817404745408...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[3.21223116140212...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[1.89440665733666...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[2.68806227425664...|\n|dbfs:/mnt/dbp8v2/...|cabbage_white_1|[2.28526925334920...|\n+--------------------+---------------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5. Sauvegarde des données sur le bucket AWS S3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8dd9376-65f4-4001-9359-230162ec3042"}}},{"cell_type":"markdown","source":["On sauvegarde nos données finales directement sur notre bucket Amazon S3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48675a52-58b7-4377-96ac-826b5621234c"}}},{"cell_type":"code","source":["# Sauvegarde des données\n#df_final.write.mode(\"overwrite\").parquet(\"/mnt/dbp8v2/output_features_parquet/df_final\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"096583f0-2453-406f-b8be-099b24fef8f3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Vérification : Est-ce-que le fichier a bien été enregistré dans le repertoire \"output_features_parquet\" de notre bucket S3 ?\n#display(dbutils.fs.ls(\"/mnt/dbp8v2/output_features_parquet/df_final\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d25af2ce-2d34-4c18-8876-3e5d707653f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Conclusion"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b423e63-8155-4b54-8eca-97f9365b0677"}}},{"cell_type":"markdown","source":["Nous avons bien réalisé les objectifs fixés :\n\n* connecter Databricks avec une instance de stockage AWS S3 (\"/mnt/dbp8v2\")\n* charger les images stockées sur un bucket AWS S3 (https://dbp8v2.s3.us-west-1.amazonaws.com/training_sample/)\n* effectuer un préprocessing de ces images (extraction des features grâce à ResNet50, resizing des images (224x224), labelisation des images )\n* effectuer une réduction de dimension (transformation des features en vecteur, détermination du nombre optimal de composantes principales, puis PCA)\n* écrire le résultat de ce processing dans un fichier accessible sur AWS S3  (https://dbp8v2.s3.us-west-1.amazonaws.com/output_features_parquet/)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d35c22d4-c550-448d-a832-a67616202894"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"notebook_p8","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4037970092288689}},"nbformat":4,"nbformat_minor":0}
